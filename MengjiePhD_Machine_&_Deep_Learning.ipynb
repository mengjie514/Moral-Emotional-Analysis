{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlWyVcWEer69"
      },
      "source": [
        "**Step 3. Modelling**\n",
        "\n",
        "Delving into machine and deep learning techniques to develop sophisticated models for moral prediction.\n",
        "\n",
        "**Machine Learning & Word Embedding**\n",
        "\n",
        "To process and learn from text data, we use various techniques that convert words into numerical forms that a machine can understand:\n",
        "\n",
        "*   **Bag-of-Words**: Creates a vocabulary from all the words in the text and counts how many times each word appears in each document. It's a straightforward approach to understand the prominence of words.\n",
        "\n",
        "*   **TF-IDF (Term Frequency-Inverse Document Frequency)**: Goes beyond simple counting by considering how unique a word is to each document. It helps in emphasizing words that are important in a document but less common in the entire corpus.\n",
        "\n",
        "*   **Word2Vec**: This method maps words into a high-dimensional space based on their usage context in the corpus. Words used in similar contexts are placed close together in this space, capturing their semantic relationships.\n",
        "\n",
        "*   **GloVe (Global Vectors for Word Representation)**: Similar to Word2Vec, GloVe also constructs a space where words with similar meanings are closer together. It's particularly effective because it simultaneously looks at global statistics of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAKOEg4i9qDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9762887-2392-4666-ea90-c770eb6ffc59"
      },
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve, confusion_matrix, multilabel_confusion_matrix, classification_report, log_loss, hamming_loss\n",
        "\n",
        "# Set display options for pandas\n",
        "pd.set_option('display.max_colwidth', None)  # Updated to None as -1 is deprecated\n",
        "\n",
        "# Mount Google Drive (specific to Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/PSIV\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_excel('output_for_R_0.50.xlsx')  # Load specific threshold data, e.g., 0.50 threshold with n = 31,277 rows\n",
        "\n",
        "# Install scikit-multilearn for multi-label classification (if not already installed)\n",
        "!pip install scikit-multilearn\n",
        "\n",
        "# Importing additional necessary libraries from scikit-learn\n",
        "from skmultilearn.problem_transform import ClassifierChain, LabelPowerset\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "seeds = [1, 43, 678, 90, 135]\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['new_clean_text_deep_stem'],  # Predictor variables\n",
        "    df.drop(['new_clean_text_deep_stem'], axis=1),  # Target variables\n",
        "    test_size=0.3,  # 70% training, 30% test\n",
        "    random_state=seeds[0],  # Use the first seed from the list\n",
        "    shuffle=True  # Shuffle the dataset before splitting\n",
        ")\n",
        "\n",
        "# Output the shape of the training and testing sets to verify the splits\n",
        "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "(21893,) (21893, 11)\n",
            "(9384,) (9384, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa0QDS7OBR2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3c5aae-b5c0-4867-9601-0f5e47649238"
      },
      "source": [
        "## Text Vectorization: Classic Bag of Words Model\n",
        "# Initialize a CountVectorizer to convert text to a matrix of token counts\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "vectorizer.fit(X_train)  # Fit the model to the training data\n",
        "\n",
        "# Transform the training and testing data using the fitted vectorizer\n",
        "X_train_vect = vectorizer.transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Define the target labels for the classification\n",
        "labels = ['care', 'harm', 'fairness', 'cheating', 'loyalty', 'betrayal', 'authority', 'subversion', 'purity', 'degradation', 'non-moral']\n",
        "\n",
        "## Function to run the machine learning pipeline\n",
        "def run_pipeline(pipeline, X_train, y_train, X_test, y_test):\n",
        "    pipeline.fit(X_train, y_train)  # Fit the pipeline to the training data\n",
        "    predictions = pipeline.predict(X_test)  # Make predictions on the testing data\n",
        "    # Print performance metrics\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "    print(\"F1 Score (Weighted):\", f1_score(y_test, predictions, average='weighted'))\n",
        "    print(\"ROC AUC Score:\", roc_auc_score(y_test, predictions))\n",
        "\n",
        "# Define and evaluate multiple classifiers\n",
        "print(\"MultinomialNB:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(MultinomialNB()))]),\n",
        "             X_train_vect, y_train, X_test_vect, y_test)\n",
        "\n",
        "print(\"LogisticRegression:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(LogisticRegression(solver='sag'))]),\n",
        "             X_train_vect, y_train, X_test_vect, y_test)\n",
        "\n",
        "print(\"LinearSVC:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(LinearSVC()))]),\n",
        "             X_train_vect, y_train, X_test_vect, y_test)\n",
        "\n",
        "print(\"XGBClassifier:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(xgb.XGBClassifier()))]),\n",
        "             X_train_vect, y_train, X_test_vect, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression:\n",
            "0.5180093776641091 0.6618914625084691 0.7682915987417605\n",
            "None\n",
            "LogisticRegression:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5908994032395567 0.6908824887974319 0.7412523984107313\n",
            "None\n",
            "LinearSVC:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5467817561807332 0.6757134553427353 0.7580540296483739\n",
            "None\n",
            "XGBClassifier:\n",
            "0.547847399829497 0.6247237474816334 0.6941535424862953\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwK7YVgf4gyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c66f71-f2af-40b8-f4c9-36f5d96c35fc"
      },
      "source": [
        "## Text Vectorization: TF-IDF\n",
        "# Initialize a TfidfVectorizer to convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    ngram_range=(1, 3),\n",
        "    norm='l2'\n",
        ")\n",
        "vectorizer.fit(X_train)  # Fit the vectorizer to the training data\n",
        "\n",
        "# Transform the training and testing data using the fitted vectorizer\n",
        "X_train_tfidf = vectorizer.transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "## Function to run machine learning pipeline and print evaluation metrics\n",
        "def run_pipeline(pipeline, X_train, y_train, X_test, y_test):\n",
        "    pipeline.fit(X_train, y_train)  # Fit the model to the training data\n",
        "    predictions = pipeline.predict(X_test)  # Predict on the testing data\n",
        "    # Print performance metrics\n",
        "    print(\"Accuracy Score:\", accuracy_score(y_test, predictions))\n",
        "    print(\"Weighted F1 Score:\", f1_score(y_test, predictions, average='weighted'))\n",
        "    print(\"ROC AUC Score:\", roc_auc_score(y_test, predictions))\n",
        "\n",
        "# Evaluating multiple classifiers\n",
        "print(\"MultinomialNB:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(MultinomialNB()))]),\n",
        "             X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "\n",
        "print(\"LogisticRegression:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(LogisticRegression(solver='sag'))]),\n",
        "             X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "\n",
        "print(\"LinearSVC:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(LinearSVC()))]),\n",
        "             X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "\n",
        "print(\"XGBClassifier:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(xgb.XGBClassifier()))]),\n",
        "             X_train_tfidf, y_train, X_test_tfidf, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB:\n",
            "0.4729326513213981 0.5588070883350609 0.6423210961337116\n",
            "None\n",
            "LogisticRegression:\n",
            "0.5680946291560103 0.6516124338278884 0.696659934083119\n",
            "None\n",
            "LinearSVC:\n",
            "0.5919650468883205 0.6953741727561449 0.7460732769913477\n",
            "None\n",
            "XGBClassifier:\n",
            "0.5563725490196079 0.6260073806199076 0.6940687860665743\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEpiTF72iD63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf78de6b-105f-45b4-d8fa-f5960055bd03"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Prepare combined dataset for Word2Vec training\n",
        "combined_df = X_train.append(X_test)\n",
        "Vocab_list = combined_df.apply(lambda x: str(x).strip().split())\n",
        "\n",
        "# Train Word2Vec model\n",
        "models = Word2Vec(Vocab_list, vector_size=100, window=5, min_count=1, workers=4)\n",
        "WordVectorz = {word: vec for word, vec in zip(models.wv.index_to_key, models.wv.vectors)}\n",
        "\n",
        "# Class to convert text data to averaged word vectors\n",
        "class AverageEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.dim = 100  # Dimensionality of the word vectors\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforms sentences to mean of the word vectors\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[word] for word in words if word in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "\n",
        "# Setting up machine learning pipelines with word vector average embedding\n",
        "print(\"LogisticRegression:\")\n",
        "run_pipeline(Pipeline([\n",
        "    (\"wordVectz\", AverageEmbeddingVectorizer(WordVectorz)),\n",
        "    (\"multilabel\", MultiOutputClassifier(LogisticRegression()))\n",
        "]), X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"LinearSVC:\")\n",
        "run_pipeline(Pipeline([\n",
        "    (\"wordVectz\", AverageEmbeddingVectorizer(WordVectorz)),\n",
        "    (\"multilabel\", MultiOutputClassifier(LinearSVC()))\n",
        "]), X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"XGBClassifier:\")\n",
        "run_pipeline(Pipeline([\n",
        "    (\"wordVectz\", AverageEmbeddingVectorizer(WordVectorz)),\n",
        "    (\"multilabel\", MultiOutputClassifier(xgb.XGBClassifier()))\n",
        "]), X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression:\n",
            "0.17604433077578857 0.19238847558186056 0.5079920173797385\n",
            "LinearSVC:\n",
            "0.20737425404944587 0.21859851818446543 0.5116604892614635\n",
            "XGBClassifier:\n",
            "0.2693947144075021 0.3128694845732286 0.5498994655478208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymY0rHax4tpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d6fa5a-ad1a-4711-9fbb-b2a202e8ac16"
      },
      "source": [
        "nltk.download('punkt')\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize tokenizer with a maximum of 5000 words\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(df['new_clean_text_deep_stem'])\n",
        "sequences = tokenizer.texts_to_sequences(df['new_clean_text_deep_stem'])\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "x = pad_sequences(sequences, maxlen=200)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x,\n",
        "    df[df.columns[1:]],  # Assuming the target variables start from the second column\n",
        "    test_size=0.3,\n",
        "    random_state=seeds[4]\n",
        ")\n",
        "\n",
        "# Load the GloVe pre-trained word vectors\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "# Function to convert sentences to vectors\n",
        "def sent2vec(s):\n",
        "    words = word_tokenize(str(s).lower())\n",
        "    M = [embeddings_dictionary.get(w, np.zeros(100)) for w in words]  # Default to zero vector if word not found\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    return v / np.sqrt((v ** 2).sum()) if np.linalg.norm(v) else np.zeros(100)\n",
        "\n",
        "# Vectorize training and testing data\n",
        "X_train = [sent2vec(x) for x in tqdm(X_train, desc=\"Vectorizing training data\")]\n",
        "X_test = [sent2vec(x) for x in tqdm(X_test, desc=\"Vectorizing testing data\")]\n",
        "\n",
        "# Machine learning pipelines\n",
        "print(\"LogisticRegression:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(LogisticRegression()))]),\n",
        "             X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"LinearSVC:\")\n",
        "run_pipeline(Pipeline([('clf', MultiOutputClassifier(LinearSVC()))]),\n",
        "             X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21893/21893 [00:23<00:00, 950.88it/s]\n",
            "100%|██████████| 9384/9384 [00:10<00:00, 935.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression:\n",
            "0.05530690537084399 0.057154021028701325 0.5032139860446211\n",
            "None\n",
            "LinearSVC:\n",
            "0.2405157715260017 0.24976911054276785 0.5106806962207272\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODW3rwdzoBWY"
      },
      "source": [
        "**Deep Learning**\n",
        "\n",
        "To boost performance in sentiment analysis within Natural Language Processing (NLP), we explore advanced deep learning techniques including:\n",
        "\n",
        "*   **DNN (Deep Neural Network)**\n",
        "*   **CNN (Convolutional Neural Network)**\n",
        "*   **LSTM (Long Short-Term Memory) & BiLSTM (Bidirectional Long Short-Term Memory)**: These models excel in processing sequential data, making them particularly suited for tasks where the order of elements is crucial, such as text processing. They are capable of remembering information over extended periods, an essential feature in NLP for preserving context in sentences or more extensive text passages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbZ4q11soBok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "bb95e8e6-efad-461c-8f0d-d97418a9bda3"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.models\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, SpatialDropout1D, Flatten, Dense, LSTM, GlobalMaxPool1D, Activation, Conv1D, Input, Bidirectional\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Tokenizer configuration\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(df['new_clean_text_deep_stem'])\n",
        "sequences = tokenizer.texts_to_sequences(df['new_clean_text_deep_stem'])\n",
        "x = pad_sequences(sequences, maxlen=200)\n",
        "print('Shape of data tensor:', x.shape)\n",
        "\n",
        "# Define seeds for reproducibility\n",
        "seeds = [1, 43, 678, 90, 135]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, df.iloc[:, 1:], test_size=0.3, random_state=seeds[0])\n",
        "print('Training data shape:', X_train.shape, y_train.shape)\n",
        "print('Testing data shape:', X_test.shape, y_test.shape)\n",
        "\n",
        "# Calculate class weights for imbalanced classes\n",
        "most_common_cat = pd.DataFrame()\n",
        "most_common_cat['cat'] = df.columns[1:]\n",
        "most_common_cat['count'] = df.iloc[:, 1:].sum().values\n",
        "most_common_cat.sort_values('count', inplace=True, ascending=False)\n",
        "most_common_cat.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Adjust class weights inversely proportional to class frequencies\n",
        "most_common_cat['class_weight'] = len(most_common_cat) / most_common_cat['count']\n",
        "class_weight = {index: most_common_cat[most_common_cat['cat'] == label]['class_weight'].values[0] for index, label in enumerate(df.columns[1:])}\n",
        "\n",
        "# Display the first few rows of the class weights DataFrame\n",
        "most_common_cat.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Shape of data tensor: (31277, 200)\n",
            "(21893, 200) (21893, 11)\n",
            "(9384, 200) (9384, 11)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2aec8368-552b-47c8-b184-0edb07b57e42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>count</th>\n",
              "      <th>class_weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>non-moral</td>\n",
              "      <td>14649</td>\n",
              "      <td>0.000751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>harm</td>\n",
              "      <td>3966</td>\n",
              "      <td>0.002774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cheating</td>\n",
              "      <td>3724</td>\n",
              "      <td>0.002954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>care</td>\n",
              "      <td>2550</td>\n",
              "      <td>0.004314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fairness</td>\n",
              "      <td>2300</td>\n",
              "      <td>0.004783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2aec8368-552b-47c8-b184-0edb07b57e42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2aec8368-552b-47c8-b184-0edb07b57e42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2aec8368-552b-47c8-b184-0edb07b57e42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         cat  count  class_weight\n",
              "0  non-moral  14649  0.000751    \n",
              "1  harm       3966   0.002774    \n",
              "2  cheating   3724   0.002954    \n",
              "3  care       2550   0.004314    \n",
              "4  fairness   2300   0.004783    "
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = y_train.shape[1]\n",
        "max_words = len(tokenizer.word_index) + 1\n",
        "maxlen = 200\n",
        "filter_length = 300\n",
        "\n",
        "# Define a simple DNN model\n",
        "def getModel_dnn():\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, 128, input_length=maxlen),\n",
        "        GlobalMaxPool1D(),\n",
        "        Dense(num_classes, activation='sigmoid'),\n",
        "    ], name=\"DNN_Model\")\n",
        "    return model\n",
        "\n",
        "# Define a CNN model\n",
        "def getModel_cnn():\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, 128, input_length=maxlen),\n",
        "        Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1),\n",
        "        GlobalMaxPool1D(),\n",
        "        Dense(num_classes, activation='sigmoid'),\n",
        "    ], name=\"CNN_Model\")\n",
        "    return model\n",
        "\n",
        "# Define an LSTM model\n",
        "def getModel_lstm():\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, 128, input_length=maxlen),\n",
        "        SpatialDropout1D(0.2),\n",
        "        LSTM(128, dropout=0.2, recurrent_dropout=0.0),\n",
        "        Dense(num_classes, activation='sigmoid'),\n",
        "    ], name=\"LSTM_Model\")\n",
        "    return model\n",
        "\n",
        "# Define a BiLSTM model\n",
        "def getModel_bilstm():\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, 128, input_length=maxlen),\n",
        "        SpatialDropout1D(0.2),\n",
        "        Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.0)),\n",
        "        Dense(num_classes, activation='sigmoid'),\n",
        "    ], name=\"BiLSTM_Model\")\n",
        "    return model\n",
        "\n",
        "# Initialize and compile the LSTM model with appropriate loss function and metrics\n",
        "training_model = getModel_lstm()\n",
        "training_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',  # Appropriate for binary classification tasks\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tfa.metrics.F1Score(num_classes=11, average='micro', threshold=0.49)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define callbacks for learning rate adjustment and model checkpointing\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(),  # Reduces learning rate when a metric has stopped improving\n",
        "    ModelCheckpoint(filepath='model-lstm.h5', save_best_only=True)  # Saves the best model observed during training\n",
        "]\n",
        "\n",
        "# Train the model with class weights, validation split, and callbacks\n",
        "history = training_model.fit(\n",
        "    X_train, y_train,\n",
        "    class_weight=class_weight,\n",
        "    epochs=6,\n",
        "    batch_size=32,\n",
        "    validation_split=0.3,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "metrics = training_model.evaluate(X_test, y_test)\n",
        "print(\"{}: {:.2f}\".format(training_model.metrics_names[1], metrics[1]))\n",
        "\n",
        "# Optional: Code to serialize model to JSON and save weights (commented out for potential use)\n",
        "# lstm_model_json = training_model.to_json()\n",
        "# with open(\"lstm_model.json\", \"w\") as json_file:\n",
        "#     json_file.write(lstm_model_json)\n",
        "# training_model.save_weights(\"lstm_model.h5\")\n",
        "# print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsjkc-b4Y-55",
        "outputId": "42c5caf8-cf0d-4a09-98e8-c9523e582a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "479/479 [==============================] - 12s 19ms/step - loss: 8.3866e-04 - binary_accuracy: 0.8932 - auc_37: 0.7350 - f1_score: 0.2260 - val_loss: 0.2702 - val_binary_accuracy: 0.9076 - val_auc_37: 0.7939 - val_f1_score: 0.3692 - lr: 0.0010\n",
            "Epoch 2/6\n",
            "479/479 [==============================] - 9s 18ms/step - loss: 6.3950e-04 - binary_accuracy: 0.9215 - auc_37: 0.8588 - f1_score: 0.4987 - val_loss: 0.1982 - val_binary_accuracy: 0.9289 - val_auc_37: 0.9047 - val_f1_score: 0.5673 - lr: 0.0010\n",
            "Epoch 3/6\n",
            "479/479 [==============================] - 8s 17ms/step - loss: 4.9952e-04 - binary_accuracy: 0.9365 - auc_37: 0.9221 - f1_score: 0.6404 - val_loss: 0.1802 - val_binary_accuracy: 0.9363 - val_auc_37: 0.9216 - val_f1_score: 0.6518 - lr: 0.0010\n",
            "Epoch 4/6\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 4.3045e-04 - binary_accuracy: 0.9453 - auc_37: 0.9437 - f1_score: 0.7083 - val_loss: 0.1717 - val_binary_accuracy: 0.9382 - val_auc_37: 0.9306 - val_f1_score: 0.6712 - lr: 0.0010\n",
            "Epoch 5/6\n",
            "479/479 [==============================] - 8s 17ms/step - loss: 3.8186e-04 - binary_accuracy: 0.9510 - auc_37: 0.9570 - f1_score: 0.7461 - val_loss: 0.1683 - val_binary_accuracy: 0.9412 - val_auc_37: 0.9329 - val_f1_score: 0.6939 - lr: 0.0010\n",
            "Epoch 6/6\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 3.4485e-04 - binary_accuracy: 0.9557 - auc_37: 0.9654 - f1_score: 0.7754 - val_loss: 0.1728 - val_binary_accuracy: 0.9402 - val_auc_37: 0.9306 - val_f1_score: 0.6909 - lr: 0.0010\n",
            "294/294 [==============================] - 2s 6ms/step - loss: 0.1722 - binary_accuracy: 0.9410 - auc_37: 0.9301 - f1_score: 0.6960\n",
            "binary_accuracy: 0.9409826993942261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and compile the BiLSTM model\n",
        "training_model = getModel_bilstm()\n",
        "training_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tfa.metrics.F1Score(num_classes=11, average='micro', threshold=0.49)  # Micro-average is used here, can be switched to 'weighted' if needed\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define callbacks for adaptive learning rate and model checkpointing\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(),  # Dynamically reduce learning rate when validation performance stalls\n",
        "    ModelCheckpoint(filepath='model-bilstm.h5', save_best_only=True)  # Save the best version of the model based on validation loss\n",
        "]\n",
        "\n",
        "# Train the BiLSTM model\n",
        "history = training_model.fit(\n",
        "    X_train, y_train,\n",
        "    class_weight=class_weight,  # Handle class imbalance\n",
        "    epochs=6,\n",
        "    batch_size=32,\n",
        "    validation_split=0.3,  # Use 30% of the data for validation\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluate the model's performance on the test dataset\n",
        "metrics = training_model.evaluate(X_test, y_test)\n",
        "print(\"{}: {:.2f}\".format(training_model.metrics_names[1], metrics[1]))  # Print the second metric (AUC or Binary Accuracy based on list order)\n",
        "\n",
        "# Optional code to serialize model to JSON and save weights (currently commented out)\n",
        "# lstm_model_json = training_model.to_json()\n",
        "# with open(\"lstm_model.json\", \"w\") as json_file:\n",
        "#     json_file.write(lstm_model_json)\n",
        "# training_model.save_weights(\"lstm_model.h5\")\n",
        "# print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBlS35KYdeo6",
        "outputId": "9c43cea3-992b-4ffe-a454-2e95084ab68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "479/479 [==============================] - 20s 34ms/step - loss: 8.2193e-04 - binary_accuracy: 0.8964 - auc_40: 0.7473 - f1_score: 0.2761 - val_loss: 0.2517 - val_binary_accuracy: 0.9176 - val_auc_40: 0.8245 - val_f1_score: 0.4404 - lr: 0.0010\n",
            "Epoch 2/6\n",
            "479/479 [==============================] - 14s 29ms/step - loss: 5.9679e-04 - binary_accuracy: 0.9265 - auc_40: 0.8799 - f1_score: 0.5486 - val_loss: 0.1838 - val_binary_accuracy: 0.9354 - val_auc_40: 0.9186 - val_f1_score: 0.6372 - lr: 0.0010\n",
            "Epoch 3/6\n",
            "479/479 [==============================] - 14s 29ms/step - loss: 4.6381e-04 - binary_accuracy: 0.9411 - auc_40: 0.9337 - f1_score: 0.6781 - val_loss: 0.1717 - val_binary_accuracy: 0.9392 - val_auc_40: 0.9307 - val_f1_score: 0.6752 - lr: 0.0010\n",
            "Epoch 4/6\n",
            "479/479 [==============================] - 14s 29ms/step - loss: 4.0159e-04 - binary_accuracy: 0.9485 - auc_40: 0.9519 - f1_score: 0.7303 - val_loss: 0.1654 - val_binary_accuracy: 0.9410 - val_auc_40: 0.9361 - val_f1_score: 0.6902 - lr: 0.0010\n",
            "Epoch 5/6\n",
            "479/479 [==============================] - 14s 29ms/step - loss: 3.5802e-04 - binary_accuracy: 0.9541 - auc_40: 0.9624 - f1_score: 0.7665 - val_loss: 0.1648 - val_binary_accuracy: 0.9413 - val_auc_40: 0.9377 - val_f1_score: 0.6971 - lr: 0.0010\n",
            "Epoch 6/6\n",
            "479/479 [==============================] - 13s 27ms/step - loss: 3.2557e-04 - binary_accuracy: 0.9581 - auc_40: 0.9694 - f1_score: 0.7894 - val_loss: 0.1712 - val_binary_accuracy: 0.9406 - val_auc_40: 0.9339 - val_f1_score: 0.6973 - lr: 0.0010\n",
            "294/294 [==============================] - 3s 9ms/step - loss: 0.1697 - binary_accuracy: 0.9409 - auc_40: 0.9345 - f1_score: 0.7000\n",
            "binary_accuracy: 0.9409242272377014\n"
          ]
        }
      ]
    }
  ]
}